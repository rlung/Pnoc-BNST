{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Homecage-odor-exposure\" data-toc-modified-id=\"Homecage-odor-exposure-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Homecage odor exposure</a></div><div class=\"lev2 toc-item\"><a href=\"#Import-data\" data-toc-modified-id=\"Import-data-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Import data</a></div><div class=\"lev3 toc-item\"><a href=\"#Import-behavior\" data-toc-modified-id=\"Import-behavior-111\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Import behavior</a></div><div class=\"lev3 toc-item\"><a href=\"#Import-calcium-imaging-data\" data-toc-modified-id=\"Import-calcium-imaging-data-112\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Import calcium imaging data</a></div><div class=\"lev2 toc-item\"><a href=\"#Save-data\" data-toc-modified-id=\"Save-data-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Save data</a></div><div class=\"lev1 toc-item\"><a href=\"#Elevated-plus-maze\" data-toc-modified-id=\"Elevated-plus-maze-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Elevated-plus maze</a></div><div class=\"lev2 toc-item\"><a href=\"#Import-data\" data-toc-modified-id=\"Import-data-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Import data</a></div><div class=\"lev3 toc-item\"><a href=\"#Import-behavioral-data\" data-toc-modified-id=\"Import-behavioral-data-211\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Import behavioral data</a></div><div class=\"lev3 toc-item\"><a href=\"#Downsample-data\" data-toc-modified-id=\"Downsample-data-212\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Downsample data</a></div><div class=\"lev3 toc-item\"><a href=\"#Import-calcium-imaging-data\" data-toc-modified-id=\"Import-calcium-imaging-data-213\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Import calcium imaging data</a></div><div class=\"lev3 toc-item\"><a href=\"#Downsample-data-(if-necessary)\" data-toc-modified-id=\"Downsample-data-(if-necessary)-214\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;</span>Downsample data (if necessary)</a></div><div class=\"lev3 toc-item\"><a href=\"#Save-data\" data-toc-modified-id=\"Save-data-215\"><span class=\"toc-item-num\">2.1.5&nbsp;&nbsp;</span>Save data</a></div><div class=\"lev1 toc-item\"><a href=\"#Headfixed-exposure\" data-toc-modified-id=\"Headfixed-exposure-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Headfixed exposure</a></div><div class=\"lev2 toc-item\"><a href=\"#Create-behavioral-data\" data-toc-modified-id=\"Create-behavioral-data-31\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Create behavioral data</a></div><div class=\"lev2 toc-item\"><a href=\"#Import-neural-data\" data-toc-modified-id=\"Import-neural-data-32\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Import neural data</a></div><div class=\"lev3 toc-item\"><a href=\"#Save-data\" data-toc-modified-id=\"Save-data-321\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Save data</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T15:06:54.953783Z",
     "start_time": "2017-11-04T15:06:51.531262Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py as h5\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import multiprocessing as mp\n",
    "import custom\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='homecage'></a>\n",
    "# Homecage odor exposure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T19:17:21.105915Z",
     "start_time": "2017-11-03T23:17:21.845Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "bin_size = 200\n",
    "n_cores = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T19:17:21.106460Z",
     "start_time": "2017-11-03T23:17:21.852Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define import function\n",
    "def import_behav(filename):\n",
    "    subj, epoch = os.path.splitext(os.path.basename(filename))[0].split('_')\n",
    "    data = custom.etho_extract(filename)\n",
    "    data.index = data.index * 1000\n",
    "    \n",
    "    return (subj, epoch), data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T19:17:21.107054Z",
     "start_time": "2017-11-03T23:17:21.858Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "# This will lock up often. Not sure why...\n",
    "# Lowered number of processes in pool and seems to work better... nope\n",
    "# Keep Excel files closed seems to do the trick... nah\n",
    "# Define pool AFTER defining all functions is the fix:\n",
    "#     https://stackoverflow.com/questions/2782961/yet-another-confusion-with-multiprocessing-error-module-object-has-no-attribu\n",
    "#     https://stackoverflow.com/questions/18947876/using-python-multiprocessing-pool-in-the-terminal-and-in-code-modules-for-django\n",
    "\n",
    "behav_files = glob.glob(behav_source)\n",
    "p = mp.Pool(processes=n_cores)\n",
    "exps, behav_import = zip(*p.map(import_behav, behav_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T19:17:21.107610Z",
     "start_time": "2017-11-03T23:17:21.865Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dataframe from all animals\n",
    "\n",
    "ts = np.arange(0, 300000, bin_size)\n",
    "subjs = np.unique([x for x, _ in exps])\n",
    "\n",
    "dfs = []\n",
    "for subj in subjs:\n",
    "    epoch_key, subj_exps = zip(*[(y, behav_import[n]) for n, (x, y) in enumerate(exps) if x == subj])\n",
    "\n",
    "    # Downsample data and data frame for subject (with each epoch)\n",
    "    subj_df = pd.DataFrame()\n",
    "    for exp, epoch_name in zip(subj_exps, epoch_key):\n",
    "        old_ts = exp.index\n",
    "        data = exp.as_matrix()\n",
    "        data_ds = custom.resample(data, old_ts, ts, method=np.nanmean)\n",
    "        epoch_df = pd.DataFrame(data_ds, columns=exp.columns, index=ts)\n",
    "        epoch_df.index = pd.MultiIndex.from_product([[epoch_name], epoch_df.index], names=['epoch', 'timestamp'])\n",
    "        subj_df = pd.concat([subj_df, epoch_df], axis=0)\n",
    "\n",
    "    subj_df.columns = pd.MultiIndex.from_product([[subj], subj_df.columns], names=['subject', 'feature'])\n",
    "    dfs.append(subj_df)\n",
    "\n",
    "behav_df = pd.concat(dfs, axis=1, names=['subject', 'feature'])\n",
    "behav_df = behav_df.sort_index(axis=1, level=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import calcium imaging data\n",
    "Each session is 1499 or 1500 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T19:17:21.108144Z",
     "start_time": "2017-11-03T23:17:21.872Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "frame_dur = 200\n",
    "epochs = ['baseline', 'h2o', 'tmt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T19:17:21.108680Z",
     "start_time": "2017-11-03T23:17:21.879Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "\n",
    "# Gather files\n",
    "trace_files = glob.glob(trace_source)\n",
    "event_files = glob.glob(event_source)\n",
    "\n",
    "# Import data\n",
    "trace_import = {\n",
    "    os.path.basename(f).split('_')[1]: np.loadtxt(f, delimiter=',')\n",
    "    for f in trace_files\n",
    "}\n",
    "event_import = {\n",
    "    os.path.basename(f).split('_')[1]: np.loadtxt(f, delimiter=',')\n",
    "    for f in event_files\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T19:17:21.109236Z",
     "start_time": "2017-11-03T23:17:21.902Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dataframes for traces and events\n",
    "\n",
    "subjs = trace_import.keys()\n",
    "trace_df = pd.DataFrame()\n",
    "event_df = pd.DataFrame()\n",
    "\n",
    "for subj in subjs:\n",
    "    img_ts = np.arange(trace_import[subj].T.shape[0]) * frame_dur\n",
    "    ts_long = np.arange(0, img_ts[-1], bin_size)\n",
    "    \n",
    "    subj_index = pd.MultiIndex.from_product([epochs, ts], names=['epoch', 'timestamps'])[:len(img_ts)]\n",
    "    \n",
    "#     trace_ds = custom.resample(trace_import[subj].T, img_ts, ts_long, method=np.nanmean)\n",
    "#     event_ds = custom.resample(event_import[subj].T, img_ts, ts_long, method=np.nanmean)\n",
    "#     subj_trace_df = pd.DataFrame(trace_ds, index=subj_index)\n",
    "#     subj_event_df = pd.DataFrame(event_ds, index=subj_index)\n",
    "    \n",
    "    subj_trace_df = pd.DataFrame(trace_import[subj].T, index=subj_index)\n",
    "    subj_event_df = pd.DataFrame(event_import[subj].T, index=subj_index)\n",
    "    \n",
    "    subj_trace_df.columns = pd.MultiIndex.from_product([[subj], subj_trace_df.columns])\n",
    "    subj_event_df.columns = pd.MultiIndex.from_product([[subj], subj_event_df.columns])\n",
    "    \n",
    "    trace_df = pd.concat([trace_df, subj_trace_df], axis=1)\n",
    "    event_df = pd.concat([event_df, subj_event_df], axis=1)\n",
    "\n",
    "neural_df = pd.concat([trace_df, event_df], axis=1, keys=['trace', 'event'], names=['datatype', 'subject', 'neuron'])\n",
    "neural_df = neural_df.sort_index(axis=1, level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T19:17:21.109769Z",
     "start_time": "2017-11-03T23:17:21.910Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check\n",
    "\n",
    "subj = np.random.choice(trace_import.keys())\n",
    "cell = np.random.choice(np.arange(trace_import[subj].shape[0]))\n",
    "\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(15, 5))\n",
    "ax[0].plot(trace_import[subj][cell]);\n",
    "ax[1].plot(neural_df[('trace', subj, cell)].as_matrix());\n",
    "ax[2].plot(trace_import[subj][cell]);\n",
    "ax[2].plot(neural_df[('trace', subj, cell)].as_matrix());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T19:17:21.110292Z",
     "start_time": "2017-11-03T23:17:21.915Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with pd.HDFStore(h5_outfile) as hf:\n",
    "    hf['behav'] = behav_df\n",
    "    hf['neural'] = neural_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='epm'></a>\n",
    "# Elevated-plus maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T19:17:21.110843Z",
     "start_time": "2017-11-03T23:17:21.922Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "behav_source = '/data/Dropbox (Stuber Lab)/We PNOC-ing/Latest PNOC Data/PNOC_EPM/PNOC_Behavior/*.xlsx'\n",
    "trace_source = '/data/Dropbox (Stuber Lab)/We PNOC-ing/Latest PNOC Data/PNOC_EPM/PNOC_Traces/*.txt'\n",
    "event_source = '/data/Dropbox (Stuber Lab)/We PNOC-ing/Latest PNOC Data/PNOC_EPM/PNOC_Spikes/*.txt'\n",
    "\n",
    "h5_outfile = '/data/Dropbox (Stuber Lab)/We PNOC-ing/Latest PNOC Data/epm-200msbin.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T19:17:21.153943Z",
     "start_time": "2017-11-03T23:17:21.932Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "bin_size = 200\n",
    "exp_dur = 600000\n",
    "n_cores = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import behavioral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T19:17:21.154751Z",
     "start_time": "2017-11-03T23:17:21.943Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define import function\n",
    "\n",
    "def import_behav(filename):\n",
    "    subj, epoch = os.path.splitext(os.path.basename(filename))[0].split('_')\n",
    "    data = custom.etho_extract(filename)\n",
    "    data.index = data.index * 1000\n",
    "    \n",
    "    return (subj, epoch), data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T19:17:21.155483Z",
     "start_time": "2017-11-03T23:17:21.952Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "\n",
    "behav_files = glob.glob(behav_source)\n",
    "p = mp.Pool(processes=n_cores)\n",
    "exps, behav_import = zip(*p.map(import_behav, behav_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T19:17:21.156227Z",
     "start_time": "2017-11-03T23:17:21.960Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Correct for 10-s extra at beginning of behavioral data\n",
    "for data in behav_import:\n",
    "    data.index -= 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T19:17:21.157218Z",
     "start_time": "2017-11-03T23:17:21.977Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dataframe from all animals\n",
    "\n",
    "ts = np.arange(0, exp_dur, bin_size)\n",
    "subjs = [x for x, _ in exps]\n",
    "\n",
    "dfs = {}  # Dictionary to store DataFrame from each animal\n",
    "for subj, data in zip(subjs, behav_import):\n",
    "    data_ds = custom.resample(data, data.index, ts, method=np.nanmean)\n",
    "    ds_df = pd.DataFrame(data_ds, columns=data.columns, index=ts)\n",
    "    ds_df.columns.names = ['feature']\n",
    "    ds_df.index.names = ['timestamp']\n",
    "    dfs[subj] = ds_df\n",
    "\n",
    "# Create DataFrame for all data\n",
    "behav_df = pd.concat(dfs, axis=1, names=['subject', 'feature'])\n",
    "behav_df = behav_df.sort_index(axis=1, level=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import calcium imaging data\n",
    "Each session is 1499 or 1500 frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T19:17:21.193960Z",
     "start_time": "2017-11-03T23:17:22.010Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "frame_dur = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T19:17:21.219219Z",
     "start_time": "2017-11-03T23:17:22.031Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gather files\n",
    "trace_files = glob.glob(trace_source)\n",
    "event_files = glob.glob(event_source)\n",
    "\n",
    "# Import data\n",
    "trace_import = {\n",
    "    os.path.basename(f).split('_')[1]: pd.DataFrame(np.loadtxt(f, delimiter=',').T)\n",
    "    for f in trace_files\n",
    "}\n",
    "event_import = {\n",
    "    os.path.basename(f).split('_')[1]: pd.DataFrame(np.loadtxt(f, delimiter=',').T)\n",
    "    for f in event_files\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T19:17:21.220387Z",
     "start_time": "2017-11-03T23:17:22.040Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trace_import['PNOC188']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T19:17:21.236096Z",
     "start_time": "2017-11-03T23:17:22.052Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "\n",
    "trace_df = pd.concat(trace_import, axis=1)\n",
    "event_df = pd.concat(event_import, axis=1)\n",
    "neural_df = pd.concat([trace_df, event_df], axis=1, keys=['trace', 'event'], names=['datatype', 'subject', 'neuron'])\n",
    "neural_df.index = np.arange(0, exp_dur, frame_dur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsample data (if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T19:17:21.273714Z",
     "start_time": "2017-11-03T23:17:22.069Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts = np.arange(0, exp_dur, bin_size)\n",
    "data_ds = custom.resample(neural_df_orig, neural_df.index, ts, method=np.nanmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T19:17:21.274906Z",
     "start_time": "2017-11-03T23:17:22.088Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neural_df = pd.DataFrame(data_ds, columns=neural_df_orig.columns, index=ts)\n",
    "neural_df.index.name = 'timestamp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-03T19:17:21.384585Z",
     "start_time": "2017-11-03T19:17:21.364477Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with pd.HDFStore(h5_outfile) as hf:\n",
    "    hf['behav'] = behav_df\n",
    "    hf['neural'] = neural_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "<a id='headfixed'></a>\n",
    "# Headfixed exposure\n",
    "Create behavioral file with `pupilize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T15:06:55.892386Z",
     "start_time": "2017-11-04T15:06:55.847979Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frame_dur = 200\n",
    "threshold = 225\n",
    "\n",
    "del_pnt = '/data/Dropbox (Stuber Lab)/We PNOC-ing/Latest PNOC Data/2P Data/del_hf-pnt.csv'\n",
    "del_tmt = '/data/Dropbox (Stuber Lab)/We PNOC-ing/Latest PNOC Data/2P Data/del_hf-tmt.csv'\n",
    "\n",
    "hf_match_file = '/data/Dropbox (Stuber Lab)/We PNOC-ing/Latest PNOC Data/2P Data/matches.csv'\n",
    "\n",
    "h5_out = '/data/Dropbox (Stuber Lab)/We PNOC-ing/Latest PNOC Data/headfixed.h5'\n",
    "\n",
    "raw_data_pnt = '/data/Dropbox (Stuber Lab)/We PNOC-ing/Latest PNOC Data/2P Data/PNOC_HFPNT/PNOC_Behavior'\n",
    "h5_out_pnt = '/data/Dropbox (Stuber Lab)/We PNOC-ing/Latest PNOC Data/hf-data-pnt.h5'\n",
    "\n",
    "raw_data_tmt = '/data/Dropbox (Stuber Lab)/We PNOC-ing/Latest PNOC Data/2P Data/PNOC_HFTMT/PNOC_Behavior'\n",
    "h5_out_tmt = '/data/Dropbox (Stuber Lab)/We PNOC-ing/Latest PNOC Data/hf-data-tmt.h5'\n",
    "\n",
    "ca_files = glob.glob('/data/Dropbox (Stuber Lab)/We PNOC-ing/Latest PNOC Data/2P Data/PNOC_HFPNT/PNOC_Traces/*.txt') + \\\n",
    "           glob.glob('/data/Dropbox (Stuber Lab)/We PNOC-ing/Latest PNOC Data/2P Data/PNOC_HFTMT/PNOC_Traces/*.txt')# + \\\n",
    "#            glob.glob('/data/Dropbox (Stuber Lab)/We PNOC-ing/Latest PNOC Data/2P Data/PNOC_HFPNT/PNOC_Spikes/*.txt') + \\\n",
    "#            glob.glob('/data/Dropbox (Stuber Lab)/We PNOC-ing/Latest PNOC Data/2P Data/PNOC_HFTMT/PNOC_Spikes/*.txt')\n",
    "\n",
    "# trace_files = glob.glob('/data/Dropbox (Stuber Lab)/We PNOC-ing/Latest PNOC Data/2P Data/PNOC_HFPNT/PNOC_Traces/*.txt') + \\\n",
    "#               glob.glob('/data/Dropbox (Stuber Lab)/We PNOC-ing/Latest PNOC Data/2P Data/PNOC_HFTMT/PNOC_Traces/*.txt')\n",
    "# event_files = glob.glob('/data/Dropbox (Stuber Lab)/We PNOC-ing/Latest PNOC Data/2P Data/PNOC_HFPNT/PNOC_Spikes/*.txt') + \\\n",
    "#               glob.glob('/data/Dropbox (Stuber Lab)/We PNOC-ing/Latest PNOC Data/2P Data/PNOC_HFTMT/PNOC_Spikes/*.txt')\n",
    "\n",
    "# trace_files_pnt = glob.glob('/data/Dropbox (Stuber Lab)/We PNOC-ing/Latest PNOC Data/2P Data/PNOC_HFPNT/PNOC_Traces/*.txt')\n",
    "# event_files_pnt = glob.glob('/data/Dropbox (Stuber Lab)/We PNOC-ing/Latest PNOC Data/2P Data/PNOC_HFPNT/PNOC_Spikes/*.txt')\n",
    "# h5_out_pnt = '/data/Dropbox (Stuber Lab)/We PNOC-ing/Latest PNOC Data/hf-data-pnt.h5'\n",
    "\n",
    "# trace_files_tmt = glob.glob('/data/Dropbox (Stuber Lab)/We PNOC-ing/Latest PNOC Data/2P Data/PNOC_HFTMT/PNOC_Traces/*.txt')\n",
    "# event_files_tmt = glob.glob('/data/Dropbox (Stuber Lab)/We PNOC-ing/Latest PNOC Data/2P Data/PNOC_HFTMT/PNOC_Spikes/*.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create behavioral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T15:48:27.169073Z",
     "start_time": "2017-11-04T15:22:39.635576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing Behavior_J53_PNT_A_P1_3.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J53_PNT_A_P2_3.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J51_PNT_B_P2_2.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J52_PNT_B_P1_3.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J54_PNT_A_P1_1.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J52_PNT_B_P2_3.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J54_PNT_A_P2_3.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J54_PNT_A_P1_3.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J55_PNT_A_P1_1.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J50_PNT_B_P1_3.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J31_PNT_B_P1_2.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J51_PNT_B_P2_3.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J54_PNT_A_P2_1.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J54_PNT_A_P2_2.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J52_PNT_B_P1_1.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J31_PNT_B_P1_1.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J55_PNT_A_P1_2.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J50_PNT_B_P1_1.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J52_PNT_B_P1_2.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J51_PNT_B_P1_1.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J51_PNT_B_P1_3.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J31_PNT_B_P1_3.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J51_PNT_B_P2_1.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J53_PNT_A_P2_1.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J53_PNT_A_P2_2.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J50_PNT_B_P1_2.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J53_PNT_A_P1_1.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J52_PNT_B_P2_1.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J54_PNT_A_P1_2.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J51_PNT_B_P1_2.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J55_PNT_A_P1_3.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J53_PNT_A_P1_2.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J52_PNT_B_P2_2.h5...\n",
      "Finished\n",
      "All done\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$h5_out_pnt\" \"$raw_data_pnt\" \"$frame_dur\" \"$threshold\"\n",
    "\n",
    "\"/data/Dropbox (Stuber Lab)/analysis/pnoc/organize_behav.py\" -n 7 -t $4 -b $3 -o \"$1\" \"$2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T16:03:13.707147Z",
     "start_time": "2017-11-04T15:48:27.228688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing Behavior_J52_TMT_A_P1_3.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J31_TMT_A_P1_2.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J53_TMT_B_P2_3.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J52_TMT_A_P1_2.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J51_TMT_A_P2_3.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J51_TMT_A_P1_1.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J52_TMT_A_P2_1.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J53_TMT_B_P2_2.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J53_TMT_B_P1_3.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J55_TMT_B_P1_3.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J53_TMT_B_P1_2.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J52_TMT_A_P2_2.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J51_TMT_A_P1_3.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J31_TMT_A_P1_1.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J55_TMT_B_P1_1.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J55_TMT_B_P1_2.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J52_TMT_A_P2_3.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J51_TMT_A_P2_2.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J50_TMT_A_P1_1.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J31_TMT_A_P1_3.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J51_TMT_A_P1_2.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J53_TMT_B_P1_1.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J50_TMT_A_P1_3.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J51_TMT_A_P2_1.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J53_TMT_B_P2_1.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J50_TMT_A_P1_2.h5...\n",
      "Finished\n",
      "Analyzing Behavior_J52_TMT_A_P1_1.h5...\n",
      "Finished\n",
      "All done\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$h5_out_tmt\" \"$raw_data_tmt\" \"$frame_dur\" \"$threshold\"\n",
    "\n",
    "\"/data/Dropbox (Stuber Lab)/analysis/pnoc/organize_behav.py\" -n 7 -t $4 -b $3 -o \"$1\" \"$2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T16:03:14.343440Z",
     "start_time": "2017-11-04T16:03:13.712233Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine datasets\n",
    "\n",
    "# Read individual dataset files\n",
    "with pd.HDFStore(h5_out_pnt, 'r') as hf_pnt, pd.HDFStore(h5_out_tmt, 'r') as hf_tmt:\n",
    "    df_behav = pd.concat(\n",
    "        [hf_pnt['behav'], hf_tmt['behav']],\n",
    "        axis=1,\n",
    "        keys=['PNT', 'TMT'],\n",
    "        names=['experiment', ] + hf_pnt['behav'].columns.names\n",
    "    )\n",
    "\n",
    "# Rename index\n",
    "df_behav = df_behav.rename(index={'ctrl': 'h2o', 'stim': 'odor'})\n",
    "\n",
    "# Combine datasets\n",
    "with pd.HDFStore(h5_out) as hf:\n",
    "    hf['behav'] = df_behav\n",
    "\n",
    "# Remove individual dataset files\n",
    "# os.remove(h5_out_pnt)\n",
    "# os.remove(h5_out_tmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import neural data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T16:03:14.350272Z",
     "start_time": "2017-11-04T16:03:14.346159Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "frames_per_epoch = 1505\n",
    "frame_period = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to check frame counts on new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T16:03:14.383306Z",
     "start_time": "2017-11-04T16:03:14.352932Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of frames for each epoch\n",
    "frame_ct = {\n",
    "    'J31_TMT_A_P1': [1505, 1505, 1505],\n",
    "    'J50_TMT_A_P1': [1505, 1505, 1505],\n",
    "    'J51_TMT_A_P1': [1504, 1504, 1504],\n",
    "    'J51_TMT_A_P2': [1504, 1504, 1504],\n",
    "    'J52_TMT_A_P1': [1505, 1505, 1505],\n",
    "    'J52_TMT_A_P2': [1505, 1505, 1504],\n",
    "    'J53_TMT_B_P1': [1505, 1505, 1505],\n",
    "    'J53_TMT_B_P2': [1505, 1505, 1505],\n",
    "    'J54_PNT_B_P1': [1505, 1505, 1505], #\n",
    "    'J54_PNT_B_P2': [1505, 1505, 1505], #\n",
    "    'J55_TMT_B_P1': [1505, 1505, 1505],\n",
    "    'J31_PNT_B_P1': [1505, 1505, 1505],\n",
    "    'J50_PNT_B_P1': [1505, 1505, 1505],\n",
    "    'J51_PNT_B_P1': [1505, 1505, 1505],\n",
    "    'J51_PNT_B_P2': [1505, 1505, 1505],\n",
    "    'J52_PNT_B_P1': [1505, 1505, 1505],\n",
    "    'J52_PNT_B_P2': [1505, 1505, 1505],\n",
    "    'J53_PNT_A_P1': [1505, 1505, 1505],\n",
    "    'J53_PNT_A_P2': [1505, 1505, 1504],\n",
    "    'J54_PNT_A_P1': [1505, 1505, 1505], #\n",
    "    'J54_PNT_A_P2': [1505, 1505, 1505], #\n",
    "    'J55_PNT_A_P1': [1505, 1505, 1505],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T16:03:27.206120Z",
     "start_time": "2017-11-04T16:03:14.384853Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "\n",
    "ca_import = {\n",
    "    tuple(os.path.splitext(os.path.basename(f))[0].split('_')): np.loadtxt(f, delimiter=',')\n",
    "    for f in ca_files\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T16:03:28.139659Z",
     "start_time": "2017-11-04T16:03:27.207550Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "\n",
    "# Create new dictionary with key for each neuron\n",
    "ca_traces = {}\n",
    "for exp, traces in ca_import.iteritems():\n",
    "    n_cells, n_frames = traces.shape\n",
    "    exp_id = frame_ct['_'.join(exp[1:])]\n",
    "    \n",
    "    epoch_split = np.split(traces, np.cumsum(exp_id)[:2], axis=1)\n",
    "    epoch_split_new = [\n",
    "        np.concatenate([epoch, np.nan * np.zeros((n_cells, frames_per_epoch - nf))], axis=1)\n",
    "        for epoch, nf in zip(epoch_split, exp_id)\n",
    "    ]\n",
    "    traces_new = np.concatenate(epoch_split_new, axis=1)\n",
    "    \n",
    "    for n, cell_trace in enumerate(traces_new):\n",
    "        ca_traces[exp + (n, )] = cell_trace\n",
    "\n",
    "# Create dataframe\n",
    "neural_df = pd.DataFrame(ca_traces)\n",
    "\n",
    "# Format columns\n",
    "neural_df.columns.names = ['data type', 'subject', 'experiment', 'order', 'plane', 'neuron']\n",
    "neural_df = neural_df.reorder_levels(['data type', 'experiment', 'subject', 'plane', 'order', 'neuron'], axis=1)\n",
    "neural_df = neural_df.sort_index(axis=1)\n",
    "\n",
    "# Format index\n",
    "neural_df.index = pd.MultiIndex.from_product(\n",
    "    [['base', 'h2o', 'odor'], np.arange(frames_per_epoch) * frame_period],\n",
    "    names=['epoch', 'time']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T16:03:28.333961Z",
     "start_time": "2017-11-04T16:03:28.141037Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove bad data\n",
    "\n",
    "delete_import = pd.concat(\n",
    "    [pd.read_csv(del_pnt, delimiter=','), pd.read_csv(del_tmt, delimiter=',')],\n",
    "    axis=1, keys=['PNT', 'TMT']\n",
    ")\n",
    "delete_import.columns = pd.MultiIndex.from_tuples(\n",
    "    [[x[0], ] + ['PNOC' + x[1].split('_')[0]] + [x[1].split('_')[1]] for x in delete_import.columns]\n",
    ")\n",
    "\n",
    "# Cells to delete from TMT dataset\n",
    "to_delete = [\n",
    "    ('Traces', ) + col + (int(x) - 1, )\n",
    "    for col in delete_import for x in delete_import[col]\n",
    "    if not np.isnan(x)\n",
    "]\n",
    "\n",
    "neural_df = neural_df.drop(to_delete, axis=1, errors='ignore')\n",
    "neural_df = neural_df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T16:03:28.807195Z",
     "start_time": "2017-11-04T16:03:28.335510Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('J31', 'P1') 64\n",
      "('J50', 'P1') 38\n",
      "('J51', 'P1') 120\n",
      "('J51', 'P2') 93\n",
      "('J52', 'P1') 179\n",
      "('J52', 'P2') 70\n",
      "('J53', 'P1') 238\n",
      "('J53', 'P2') 168\n",
      "No TMT experiments found for ('J54', 'P1')\n",
      "No TMT experiments found for ('J54', 'P2')\n",
      "('J55', 'P1') 76\n"
     ]
    }
   ],
   "source": [
    "# Match cells\n",
    "# Iterate over experiments, choosing only cells in peanut oil ones. Assign number based on match file.\n",
    "\n",
    "match_import = pd.read_csv(hf_match_file, delimiter=',', header=0)\n",
    "match_import -= 1  # start indexing at 0\n",
    "match_import.columns = pd.MultiIndex.from_tuples(\n",
    "    [[x.split('_')[0], x.split('_')[1]] for x in match_import.columns],\n",
    "    names=['subject', 'plane']\n",
    ")\n",
    "\n",
    "neural_df_new = []  # new dataframe with updated matched cell labelling\n",
    "for grp, grp_df in neural_df.groupby(level=['subject', 'plane'], axis=1):\n",
    "    try:\n",
    "        n_tmt_cells = max([x[-1] for x in grp_df.xs('TMT', axis=1, level='experiment')]) + 1\n",
    "        print grp, n_tmt_cells\n",
    "    except ValueError:\n",
    "        print('No TMT experiments found for {}'.format(grp))\n",
    "        continue\n",
    "    key = match_import[grp]\n",
    "\n",
    "    col = grp_df.columns.tolist()\n",
    "    for n, x in enumerate(col):\n",
    "        if 'PNT' in x:\n",
    "            neuron_id = x[-1]\n",
    "            matched_cell = key[neuron_id]\n",
    "            if not np.isnan(matched_cell):\n",
    "                col[n] = x[:-1] + (int(matched_cell), )\n",
    "            else:\n",
    "                col[n] = x[:-1] + (x[-1] + n_tmt_cells, )\n",
    "\n",
    "    grp_df.columns = pd.MultiIndex.from_tuples(col, names=grp_df.columns.names)\n",
    "    neural_df_new.append(grp_df)\n",
    "\n",
    "neural_df_matched = pd.concat(neural_df_new, axis=1).sort_index(axis=1)\n",
    "\n",
    "# # Following doesn't work...\n",
    "# def match_cells(paired_df):\n",
    "#     ''' Rename cells based on match from CSV file\n",
    "#     Doesn't work because `groupby` won't change column names on original dataframe... -_-\n",
    "#     '''\n",
    "#     n_tmt_cells = max([x[-1] for x in paired_df.xs('TMT', axis=1, level='odor')]) + 1\n",
    "#     key = match_import[paired_df.columns[0][2:4]]\n",
    "\n",
    "#     col = paired_df.columns.tolist()\n",
    "#     for n, x in enumerate(col):\n",
    "#         if 'PNT' in x:\n",
    "#             neuron_id = x[-1]\n",
    "#             if not np.isnan(key[neuron_id]):\n",
    "#                 col[n] = x[:-1] + (int(key[neuron_id]), )\n",
    "#             else:\n",
    "#                 col[n] = x[:-1] + (x[-1] + n_tmt_cells, )\n",
    "\n",
    "#     paired_df.columns = pd.MultiIndex.from_tuples(col, names=paired_df.columns.names)\n",
    "#     return paired_df\n",
    "\n",
    "# neural_df = neural_df.groupby(level=['subject', 'plane'], axis=1).apply(match_cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T16:03:28.956396Z",
     "start_time": "2017-11-04T16:03:28.808526Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with pd.HDFStore(h5_out) as hf:\n",
    "    hf['neural'] = neural_df_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T16:03:28.966986Z",
     "start_time": "2017-11-04T16:03:28.958361Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to_delete = [\n",
    "#     ('Traces', 'TMT', 'PNOCJ51', 'P2', 3),\n",
    "#     ('Traces', 'TMT', 'PNOCJ51', 'P2', 5),\n",
    "#     ('Traces', 'TMT', 'PNOCJ51', 'P2', 9),\n",
    "#     ('Traces', 'TMT', 'PNOCJ31', 'P1', 53),\n",
    "#     ('Traces', 'TMT', 'PNOCJ31', 'P1', 60),\n",
    "#     ('Traces', 'TMT', 'PNOCJ50', 'P1', 30),\n",
    "#     ('Traces', 'TMT', 'PNOCJ50', 'P1', 37),\n",
    "#     ('Traces', 'PNT', 'PNOCJ51', 'P1', 61),\n",
    "#     ('Traces', 'PNT', 'PNOCJ52', 'P1', 40),\n",
    "#     ('Traces', 'PNT', 'PNOCJ52', 'P1', 41),\n",
    "#     ('Traces', 'PNT', 'PNOCJ52', 'P1', 73),\n",
    "#     ('Traces', 'PNT', 'PNOCJ52', 'P2', 8),\n",
    "#     ('Traces', 'PNT', 'PNOCJ31', 'P1', 18),\n",
    "#     ('Traces', 'PNT', 'PNOCJ31', 'P1', 31),\n",
    "#     ('Traces', 'PNT', 'PNOCJ31', 'P1', 37),\n",
    "#     ('Traces', 'PNT', 'PNOCJ31', 'P1', 38),\n",
    "#     ('Traces', 'PNT', 'PNOCJ31', 'P1', 42),\n",
    "#     ('Traces', 'PNT', 'PNOCJ31', 'P1', 57),\n",
    "#     ('Traces', 'PNT', 'PNOCJ31', 'P1', 62),\n",
    "#     ('Traces', 'PNT', 'PNOCJ31', 'P1', 70),\n",
    "#     ('Traces', 'PNT', 'PNOCJ31', 'P1', 75),\n",
    "#     ('Traces', 'PNT', 'PNOCJ31', 'P1', 81),\n",
    "#     ('Traces', 'PNT', 'PNOCJ31', 'P1', 83),\n",
    "#     ('Traces', 'PNT', 'PNOCJ31', 'P1', 96),\n",
    "#     ('Traces', 'PNT', 'PNOCJ31', 'P1', 105),\n",
    "#     ('Traces', 'PNT', 'PNOCJ31', 'P1', 129),\n",
    "#     ('Traces', 'PNT', 'PNOCJ50', 'P1', 24),\n",
    "#     ('Traces', 'PNT', 'PNOCJ50', 'P1', 33),\n",
    "#     ('Traces', 'PNT', 'PNOCJ50', 'P1', 37),\n",
    "#     ('Traces', 'PNT', 'PNOCJ50', 'P1', 38),\n",
    "#     ('Traces', 'PNT', 'PNOCJ50', 'P1', 39),\n",
    "#     ('Traces', 'PNT', 'PNOCJ50', 'P1', 42),\n",
    "#     ('Traces', 'PNT', 'PNOCJ50', 'P1', 46),\n",
    "#     ('Traces', 'PNT', 'PNOCJ50', 'P1', 47),\n",
    "#     ('Traces', 'PNT', 'PNOCJ50', 'P1', 50),\n",
    "#     ('Traces', 'PNT', 'PNOCJ50', 'P1', 51),\n",
    "#     ('Traces', 'PNT', 'PNOCJ50', 'P1', 52),\n",
    "#     ('Traces', 'PNT', 'PNOCJ50', 'P1', 53),\n",
    "#     ('Traces', 'PNT', 'PNOCJ50', 'P1', 55),\n",
    "#     ('Traces', 'PNT', 'PNOCJ50', 'P1', 56),\n",
    "#     ('Traces', 'PNT', 'PNOCJ50', 'P1', 57),\n",
    "#     ('Traces', 'PNT', 'PNOCJ50', 'P1', 58),\n",
    "#     ('Traces', 'PNT', 'PNOCJ50', 'P1', 60),\n",
    "#     ('Traces', 'PNT', 'PNOCJ50', 'P1', 61),\n",
    "#     ('Traces', 'PNT', 'PNOCJ50', 'P1', 67),\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "228px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "739px",
    "left": "0px",
    "right": "1317px",
    "top": "106px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
